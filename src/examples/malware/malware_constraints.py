import logging
import pickle
from typing import Tuple, Union

import autograd.numpy as anp
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

from src.attacks.moeva2.constraints.constraints import Constraints


class MalwareConstraints(Constraints):
    def fix_features_types(self, x) -> Union[np.ndarray, tf.Tensor]:
        return x

    def __init__(
        self,
        feature_path: str,
        constraints_path: str,
    ):
        self._provision_constraints_min_max(constraints_path)
        self._provision_feature_constraints(feature_path)
        self._fit_scaler()
        with open("./data/malware/section_names_idx.pkl", "rb") as f:
            self.section_names_idx = pickle.load(f)

        with open("./data/malware/imports_idx.pkl", "rb") as f:
            self.imports_idx = pickle.load(f)

        with open("./data/malware/dll_imports_idx.pkl", "rb") as f:
            self.dll_imports_idx = pickle.load(f)

        with open("./data/malware/freq_idx.pkl", "rb") as f:
            self.freq_idx = pickle.load(f)

    def _fit_scaler(self) -> None:
        self._scaler = MinMaxScaler(feature_range=(0, 1))
        min_c, max_c = self.get_constraints_min_max()
        self._scaler = self._scaler.fit([min_c, max_c])

    @staticmethod
    def _date_feature_to_month(feature):
        return np.floor(feature / 100) * 12 + (feature % 100)

    def evaluate_tf2(self, x):
        tol = 1e-3

        # NumberOfSections equals the sum of sections names not set to 'none'(label encoded to 832)
        count = tf.zeros(x.shape[0])
        for i in self.section_names_idx:
            count = count + tf.cast(x[:, i] != 832, tf.float32)
        g1 = tf.math.abs(x[:, 12893] - count)

        g2 = x[:, 13956] - x[:, 10840]

        # The value for FileAlignment should be a power of 2
        m = x[:, 13956]
        g3 = tf.math.abs(
            tf.where(m <= 0.0, tf.zeros(m.shape[0]), tf.math.log(m) / tf.math.log(2.0))
            % 1
        )

        # # api_import_nb is higher than the sum of total imports that we have considered as features
        count = tf.zeros(x.shape[0])
        for i in self.imports_idx:
            count = count + x[:, i]
        g4 = count - x[:, 271]
        #
        # # api_dll_nb is higher than the sum of total dll that we have considered as features
        count = tf.zeros(x.shape[0])
        for i in self.dll_imports_idx:
            count = count + x[:, i]
        g5 = count - x[:, 8607]

        # # Sum of individual byte frequencies is equal to 1. There is a small  difference due to rounding effect
        count = tf.zeros(x.shape[0])
        for i in self.freq_idx:
            count = count + x[:, i]
        g6 = tf.math.abs(1 - count)

        # g7
        count = tf.zeros(x.shape[0])
        for i in self.freq_idx:
            logarithm = tf.math.log(x[:, i]) / tf.math.log(2.0)
            logarithm = tf.where(
                tf.math.is_inf(-logarithm), tf.zeros(logarithm.shape[0]), logarithm
            )
            count = count + x[:, i] * logarithm

        g7 = tf.math.abs(x[:, 23549] + count)

        constraints = tf.stack([g1, g2, g3, g4, g5, g6, g7], 1)

        constraints = tf.clip_by_value(constraints - tol, 0, tf.constant(np.inf))

        return constraints

    def evaluate(self, x: np.ndarray, use_tensors: bool = False):
        # ----- PARAMETERS

        if use_tensors:
            return self.evaluate_tf2(x)

        tol = 1e-3
        # should write a function in utils for this part

        # NumberOfSections equals the sum of sections names not set to 'none'(label encoded to 832)
        g1 = np.absolute(
            x[:, 12893] - np.count_nonzero(x[:, self.section_names_idx] != 832, axis=1)
        )
        #
        # # header_FileAlignment < header_SectionAlignment
        g2 = x[:, 13956] - x[:, 10840]

        # The value for FileAlignment should be a power of 2
        m = x[:, 13956]
        m = np.array(m, dtype=np.float)
        g3 = np.absolute(np.log2(m, out=np.zeros_like(m), where=(m != 0)) % 1 - 0)

        # # api_import_nb is higher than the sum of total imports that we have considered as features
        g4 = np.sum(x[:, self.imports_idx], axis=1) - x[:, 271]
        #
        # # api_dll_nb is higher than the sum of total dll that we have considered as features
        g5 = np.sum(x[:, self.dll_imports_idx], axis=1) - x[:, 8607]

        # # Sum of individual byte frequencies is equal to 1. There is a small  difference due to rounding effect
        g6 = np.absolute(1 - np.sum(x[:, self.freq_idx], axis=1))

        # FileEntropy is related to freqbytes through Shanon entropy
        m = x[:, self.freq_idx]
        m = np.array(m, dtype=np.float)
        # Log of freq_idx
        logarithm = np.log2(m, out=np.zeros_like(m), where=(m != 0))
        g7 = np.absolute(x[:, 23549] + np.sum(x[:, self.freq_idx] * logarithm, axis=1))

        constraints = anp.column_stack([g1, g2, g3, g4, g5, g6, g7])
        constraints[constraints <= tol] = 0.0

        return constraints

    def get_nb_constraints(self) -> int:
        return 7

    def normalise(self, x: np.ndarray) -> np.ndarray:
        return self._scaler.transform(x)

    def get_constraints_min_max(self) -> Tuple[np.ndarray, np.ndarray]:
        return self._constraints_min, self._constraints_max

    def get_mutable_mask(self) -> np.ndarray:
        return self._mutable_mask

    def get_feature_min_max(self, dynamic_input=None) -> Tuple[np.ndarray, np.ndarray]:

        # By default min and max are the extreme values
        feature_min = np.array([0.0] * self._feature_min.shape[0])
        feature_max = np.array([0.0] * self._feature_max.shape[0])

        # Creating the mask of value that should be provided by input
        min_dynamic = self._feature_min.astype(str) == "dynamic"
        max_dynamic = self._feature_max.astype(str) == "dynamic"

        # Replace de non dynamic value by the value provided in the definition
        feature_min[~min_dynamic] = self._feature_min[~min_dynamic]
        feature_max[~max_dynamic] = self._feature_max[~max_dynamic]

        # If the dynamic input was provided, replace value for output, else do nothing (keep the extreme values)
        if dynamic_input is not None:
            feature_min[min_dynamic] = dynamic_input[min_dynamic]
            feature_max[max_dynamic] = dynamic_input[max_dynamic]

        # Raise warning if dynamic input waited but not provided
        dynamic_number = min_dynamic.sum() + max_dynamic.sum()
        if dynamic_number > 0 and dynamic_input is None:
            logging.getLogger().warning(
                f"{dynamic_number} feature min and max are dynamic but no input were provided."
            )

        return feature_min, feature_max

    def get_feature_type(self) -> np.ndarray:
        return self._feature_type

    def _provision_feature_constraints(self, path: str) -> None:
        df = pd.read_csv(path, low_memory=False)
        self._feature_min = df["min"].to_numpy()
        self._feature_max = df["max"].to_numpy()
        self._mutable_mask = df["mutable"].to_numpy()
        self._feature_type = df["type"].to_numpy()

    def _provision_constraints_min_max(self, path: str) -> None:
        df = pd.read_csv(path, low_memory=False)
        self._constraints_min = df["min"].to_numpy()
        self._constraints_max = df["max"].to_numpy()
        self._fit_scaler()


class MalwareConstraintsFast(MalwareConstraints):
    def __init__(self):
        feature_path = "./data/malware/features.csv"
        constraints_path = "./data/malware/constraints.csv"
        super().__init__(feature_path, constraints_path)
